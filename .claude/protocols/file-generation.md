# ðŸ”„ Self-Evaluating File Generation Protocol

## Overview
This protocol ensures every file generated by AI undergoes immediate self-evaluation, validation, and improvement before acceptance.

## The Generation Loop

### Phase 1: Initial Generation
```
1. Receive file requirements
2. Check knowledge base and patterns
3. Generate initial implementation
4. STOP - Do not proceed without evaluation
```

### Phase 2: Self-Evaluation
The AI must immediately evaluate its own output against:

#### Technical Checklist
- [ ] Does this code compile/parse without errors?
- [ ] Are all imports/dependencies valid and available?
- [ ] Is error handling comprehensive?
- [ ] Are there potential security vulnerabilities?
- [ ] Does it follow established patterns?

#### Logical Analysis
- [ ] Does this solve the stated problem?
- [ ] Are there edge cases not handled?
- [ ] Could this cause issues at scale?
- [ ] Is the approach optimal or just functional?

#### Knowledge Verification
```prompt
"I just generated [FILE]. Let me verify:
1. Check against documentation: [SPECIFIC CHECKS]
2. Search for similar implementations: [SEARCH TERMS]
3. Validate best practices for: [TECHNOLOGY/PATTERN]"
```

### Phase 3: Research & Validation

If ANY concerns arise during self-evaluation:

1. **Stop Generation**
   ```
   "I've identified potential issues with my implementation.
   Let me research before proceeding..."
   ```

2. **Targeted Research**
   - Search for specific patterns: `"best practice [TECHNOLOGY] [SPECIFIC FEATURE]"`
   - Check recent updates: `"[LIBRARY] 2025 changes breaking"`
   - Validate approach: `"[PATTERN] production issues common mistakes"`

3. **Document Findings**
   ```
   Research Results:
   - Finding 1: [WHAT I LEARNED]
   - Finding 2: [BETTER APPROACH]
   - Finding 3: [COMMON PITFALL TO AVOID]
   ```

### Phase 4: Iterative Improvement

Based on self-evaluation and research:

1. **Identify Specific Improvements**
   ```
   Improvements needed:
   1. Line 23-45: Refactor error handling to include [SPECIFIC CASES]
   2. Line 67: Replace deprecated method with [NEW METHOD]
   3. Architecture: Add abstraction layer for [REASON]
   ```

2. **Apply Improvements**
   - Make changes incrementally
   - Re-evaluate after each change
   - Document why each change was made

3. **Validate Improvements**
   ```
   Validation checks:
   - [ ] Original issues resolved
   - [ ] No new issues introduced
   - [ ] Code is cleaner/more maintainable
   - [ ] Performance implications considered
   ```

### Phase 5: Final Polish

Only after improvements are validated:

1. **Code Quality Polish**
   - Consistent naming conventions
   - Comprehensive comments/documentation
   - Remove any redundant code
   - Optimize imports

2. **Test Generation**
   ```
   Generate tests that:
   - Cover happy path
   - Test each error condition
   - Validate edge cases
   - Check performance boundaries
   ```

3. **Documentation**
   ```
   Document:
   - Purpose and usage
   - Design decisions made
   - Potential future improvements
   - Known limitations
   ```

## Evaluation Prompts

### Initial Self-Check
```
"I've generated [FILE]. Let me evaluate:
1. Syntax and structure check...
2. Logic and algorithm review...
3. Best practices validation...
4. Security assessment...

Issues found: [LIST] / No issues found"
```

### Research Prompt
```
"Before finalizing, I need to verify:
1. Is [APPROACH] the current best practice?
2. Are there known issues with [PATTERN]?
3. How do production systems handle [FEATURE]?

Researching now..."
```

### Improvement Prompt
```
"Based on my evaluation and research:
1. Current implementation has [ISSUES]
2. Better approach would be [SOLUTION]
3. Making improvements now...

[SHOW SPECIFIC CHANGES]"
```

### Final Validation
```
"Final validation complete:
âœ“ All identified issues resolved
âœ“ Code follows best practices
âœ“ Tests cover edge cases
âœ“ Documentation is comprehensive
âœ“ Ready for human review"
```

## Red Flags That Require Research Mode

If ANY of these occur, switch to Research Mode immediately:

1. **Uncertainty Indicators**
   - "This might work..."
   - "I think this is correct..."
   - "This should handle..."

2. **Knowledge Gaps**
   - Unfamiliar library/framework
   - Complex architectural decision
   - Performance-critical code

3. **Error Patterns**
   - Same error occurring multiple times
   - Errors in areas just modified
   - Cascading failures

4. **Complexity Threshold**
   - File > 200 lines
   - Multiple integration points
   - State management logic
   - Concurrent operations

## Success Metrics

Track these for continuous improvement:

- **First-Pass Success Rate**: % of files needing no improvements
- **Issues Caught**: Number found in self-evaluation
- **Research Frequency**: How often research mode triggered
- **Improvement Impact**: Measurable quality increase
- **Human Rejection Rate**: Files rejected in review

## Remember

> "Every file is a reflection of our standards. Generate thoughtfully, evaluate ruthlessly, improve continuously."

The goal isn't speed - it's excellence. A file that takes 5 iterations to get right is better than one that ships with hidden flaws. 